syntax = "proto2";
package raft_cmdpb;

import "metapb.proto";
import "errorpb.proto";
import "raftpb.proto";

message GetRequest {
    optional bytes key = 1;
}

message GetResponse {
    optional bytes value = 1;
}

message SeekRequest {
    optional bytes key = 1;
}

message SeekResponse {
    optional bytes key   = 1;
    optional bytes value = 2;
}

message PutRequest {
    optional bytes key   = 1;
    optional bytes value = 2;
}

message PutResponse {
    
}

message DeleteRequest {
    optional bytes key = 1;
}

message DeleteResponse {
    
}

message SnapRequest {
    
}

message SnapResponse {
    optional metapb.Region region = 1;
}

enum CmdType {
    Invalid     = 0;
    Get         = 1;
    Seek        = 2;
    Put         = 3;
    Delete      = 4;
    Snap        = 5;
}

message Request {
    optional CmdType        cmd_type    = 1;
    optional GetRequest     get         = 2;
    optional SeekRequest    seek        = 3;
    optional PutRequest     put         = 4;
    optional DeleteRequest  delete      = 5;
    optional SnapRequest    snap        = 6;
}

message Response {
    optional CmdType        cmd_type    = 1;
    optional GetResponse    get         = 2;
    optional SeekResponse   seek        = 3;
    optional PutResponse    put         = 4;
    optional DeleteResponse delete      = 5;
    optional SnapResponse   snap        = 6;
}

message ChangePeerRequest {
    optional raftpb.ConfChangeType change_type = 1;
    optional metapb.Peer peer                  = 2;
}

message ChangePeerResponse {
    optional metapb.Region region = 1;
}

message SplitRequest {
    // The split_key must be in the been splitting region. 
    // If the split_key is none, we will choose a proper key
    // to split the region in half.
    optional bytes split_key                  = 1;
    // We split the region into two, first uses the origin 
    // parent region id, and the second uses the new_region_id.
    // We must guarantee that the new_region_id is global unique.
    optional uint64 new_region_id             = 2;
    // The peer ids for the new split region. 
    repeated uint64 new_peer_ids              = 3;
}

message SplitResponse {
    optional metapb.Region left  = 1;
    optional metapb.Region right = 2;
}

message CompactLogRequest {
    optional uint64 compact_index = 1;
}

message CompactLogResponse {
    
}

enum AdminCmdType {
    InvalidAdmin    = 0;
    ChangePeer      = 1;
    Split           = 2;
    CompactLog      = 3;
}

message AdminRequest {
    optional AdminCmdType cmd_type          = 1;
    optional ChangePeerRequest change_peer  = 2;
    optional SplitRequest split             = 3;
    optional CompactLogRequest compact_log  = 4;
}

message AdminResponse {
    optional AdminCmdType cmd_type          = 1;
    optional ChangePeerResponse change_peer = 2;
    optional SplitResponse split            = 3;
    optional CompactLogResponse compact_log = 4;  
}

// For get the leader of the region.
message RegionLeaderRequest {
    
}

message RegionLeaderResponse {
    optional metapb.Peer leader 	= 1;
}

// For getting more information of the region.
// We add some admin operations (ChangePeer, Split...) into the pb job list,
// then pd server will peek the first one, handle it and then pop it from the job lib. 
// But sometimes, the pd server may crash before popping. When another pd server
// starts and finds the job is running but not finished, it will first check whether
// the raft server already has handled this job.
// E,g, for ChangePeer, if we add Peer10 into region1 and find region1 has already had
// Peer10, we can think this ChangePeer is finished, and can pop this job from job list
// directly.
message RegionDetailRequest {

}

message RegionDetailResponse {
    optional metapb.Region region = 1;
    optional metapb.Peer leader   = 2;
}

message StoreStatsRequest {
    
}

message StoreStatsResponse {
    // Capacity is the max size for this store. 
    // If the store used size >= capacity, pd won't choose this store
    // for later auto-balance.
    // Now we don't handle capacity and used_size, will do it later. 
    optional uint64 capacity            = 1;
    optional uint64 used_size           = 2;

    optional uint64 region_count        = 3;

    // Leader peer count the store has. 
    optional uint64 leader_count        = 4;

    // TODO: add more statistics. 
}

enum StatusCmdType {
    InvalidStatus    = 0;
    RegionLeader     = 1;
    RegionDetail     = 2;
    StoreStats       = 3;
}

message StatusRequest {
    optional StatusCmdType cmd_type               = 1;
    optional RegionLeaderRequest region_leader    = 2;
    optional RegionDetailRequest region_detail    = 3;
    optional StoreStatsRequest store_stats        = 4;
}

message StatusResponse {
    optional StatusCmdType cmd_type               = 1;
    optional RegionLeaderResponse region_leader   = 2;
    optional RegionDetailResponse region_detail   = 3;
    optional StoreStatsResponse store_stats       = 4;
}

message RaftRequestHeader {
    optional uint64 region_id                   = 1;
    optional metapb.Peer peer                   = 2;
    // true for read linearization
    optional bool read_quorum                   = 3;
    // 16 bytes, to distinguish request.  
    optional bytes uuid                         = 4;
    
    optional metapb.RegionEpoch region_epoch    = 5;
}

message RaftResponseHeader {
    optional errorpb.Error error  = 1;
    optional bytes uuid           = 2;
    optional uint64 current_term  = 3;
}

message RaftCmdRequest {
    optional RaftRequestHeader header     = 1;
    // We can't enclose normal requests and administrator request
    // at same time. 
    repeated Request requests             = 2; 
    optional AdminRequest admin_request   = 3; 
    optional StatusRequest status_request = 4;
}

message RaftCmdResponse {
    optional RaftResponseHeader header      = 1;
    repeated Response responses             = 2;
    optional AdminResponse admin_response   = 3;
    optional StatusResponse status_response = 4;
}
